# LLM_Practise

# ğŸ“– Large Language Model (LLM) Practice with Turkish NLP

## ğŸš€ Overview
This repository demonstrates my exploration of **Large Language Models (LLM)** by implementing fundamental **Natural Language Processing (NLP)** techniques on a Turkish novel. The study includes **tokenization, word embeddings, and transformers** with a focus on **multi-head self-attention** to generate meaningful Turkish sentences.

## ğŸ“Œ Key Topics Covered
- **Tokenization**: Implementing character, subword, and word-based tokenization.
- **Word Embeddings**: Experimenting with **Word2Vec, FastText, and Transformer-based embeddings**.
- **Transformer Architecture**: Implementing a custom Transformer model.
- **Multi-Head Self-Attention**: Capturing contextual meaning from a Turkish text.
- **Decoding Turkish Sentences**: Generating coherent and meaningful sentences.


## ğŸ”¥ Implementation Details
### 1ï¸âƒ£ **Tokenization**
Implemented different tokenization methods:
- **Word-based Tokenization** (NLTK, SpaCy)
- **Subword Tokenization** (Byte-Pair Encoding, WordPiece, SentencePiece)
- **Character-level Tokenization**

### 2ï¸âƒ£ **Word Embeddings**
Explored multiple embedding techniques:
- **Word2Vec** (Continuous Bag of Words & Skip-gram)
- **FastText** (Subword-based embeddings for rich morphology)
- **Transformer-based Embeddings** (BERT, GPT, or custom model)

### 3ï¸âƒ£ **Custom Transformer Model**
Built a Transformer model from scratch, including:
- **Multi-Head Self-Attention Mechanism**
- **Positional Encoding**
- **Feedforward Network**
- **Layer Normalization & Dropout**
- **Decoder to generate meaningful Turkish sentences**

## ğŸ—ï¸ Model Training & Evaluation
1. **Train on a Turkish novel dataset**
2. **Evaluate perplexity, BLEU, and sentence coherence**
3. **Fine-tune embeddings and self-attention weights**

## ğŸ“Š Results & Insights
- **Effect of different tokenization strategies**
- **Comparison of word embeddings and contextual representations**
- **Performance of self-attention for Turkish sentence coherence**


## ğŸ¯ Future Work
- Fine-tune **pretrained Transformer models (BERT, GPT-3, Llama)**
- Improve **context understanding for Turkish NLP**
- Apply **RAG (Retrieval-Augmented Generation) for Turkish QA**

## ğŸ“œ References
- Vaswani et al.,  2017: *Attention Is All You Need*
- Mikolov et al.,  2013: *Word2Vec*
- Andrej Karpathy, 2023: *Let's build GPT: from scratch, in code, spelled out.*


## ğŸ¤ Contributions & Feedback
If you have any suggestions or want to collaborate, feel free to open an issue or pull request! ğŸš€
